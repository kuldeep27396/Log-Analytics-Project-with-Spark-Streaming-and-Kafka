{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf46c5a",
   "metadata": {
    "id": "30147e2a"
   },
   "source": [
    "# <font color=blue><center>Log Processing Using Lambda Architecture</center></font>\n",
    "## Agenda\n",
    "\n",
    "### Why?\n",
    "Web server log analysis can offer important insights on everything from security to customer service to SEO. The information collected in web server logs can help you with:\n",
    "- Network troubleshooting efforts\n",
    "- Development and quality assurance\n",
    "- Identifying and understanding security issues\n",
    "- Customer service\n",
    "- Maintaining compliance with both government and corporate policies\n",
    "\n",
    "### Architecture\n",
    "- Overview of data flow\n",
    "- Tech Stack\n",
    "- End result\n",
    "\n",
    "### Environment Setup\n",
    "- AWS EC2 instance and security group creation\n",
    "- Docker installation and running\n",
    "- Usage of docker-composer and starting all the tools\n",
    "- How to access tools in local machine\n",
    "\n",
    "### Log Preprocessing\n",
    "- Walk thru Common Log Format\n",
    "- Parsing Log file\n",
    "- Data Cleaning\n",
    "- Fix the rows with null content_size\n",
    "\n",
    "### Extraction\n",
    "- Download data from Kaggle\n",
    "- Push data using NiFi\n",
    "- Creating Kafka topic and publishing log data to it\n",
    "\n",
    "### Transformation and Load\n",
    "- Schema creation\n",
    "- Reading data from Kafka as Streaming Dataframe\n",
    "- Extraction and transformation of log data\n",
    "- Continuous data load to Cassandra\n",
    "\n",
    "### Visualization\n",
    "- Multipage Dash Application development\n",
    "- Realtime Dashboard\n",
    "- Hourly Dashboard\n",
    "- Daily Dashboard\n",
    "\n",
    "### Code walkthrough\n",
    "- Log Preprocessing\n",
    "- Log Listener\n",
    "- Log Visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9adfd9f",
   "metadata": {
    "id": "e7eaf504"
   },
   "source": [
    "## <font color=blue>Architecture</font>\n",
    "### Overview of data flow\n",
    "#### Data Flow Architecture\n",
    "![alt text](log_processing_using_lambda_architecture.png)\n",
    "### Tech Stack\n",
    "* AWS EC2\n",
    "* Docker\n",
    "* Jupyter Lab\n",
    "* Spark Structured Streaming\n",
    "* NiFi\n",
    "* Kafka\n",
    "* Python\n",
    "* Cassandra\n",
    "* Plotly\n",
    "* Dash\n",
    "\n",
    "### End result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fe56a",
   "metadata": {
    "id": "bd740e6f"
   },
   "source": [
    "## <font color=blue>Environment Setup</font>\n",
    "### AWS EC2 instance and security group creation\n",
    "- t2.xlarge instance\n",
    "- 32GB of storage recommended\n",
    "- Allow ports 4000 - 38888\n",
    "- Connect to ec2 via ssh\n",
    " <code>ssh -i \"D:\\path\\to\\private\\key.pem\" user@Public_DNS</code>\n",
    " <br/>Example:<code>ssh -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\" ec2-user@ec2-54-203-235-65.us-west-2.compute.amazonaws.com</code><br/>\n",
    "- Port forwarding \n",
    " <code>ssh -i \"D:\\path\\to\\private\\key.pem\" user@Public_DNS</code>\n",
    " <br/>Example:<code>ssh -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\" ec2-user@ec2-34-208-254-29.us-west-2.compute.amazonaws.com -L 2081:localhost:2041 -L 4888:localhost:4888 -L 2080:localhost:2080 -L 8050:localhost:8050 -L 4141:localhost:4141</code><br/>\n",
    "- Copy from local to ec2\n",
    "  <code>scp -r -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\"</code>\n",
    "  <br/>Example:<code>scp -r -i \"D:\\Users\\pyerravelly\\Desktop\\twitter_analysis.pem\" D:\\Users\\pyerravelly\\Downloads\\spark-standalone-cluster-on-docker-master\\build\\docker\\docker-exp ec2-user@ec2-34-208-254-29.us-west-2.compute.amazonaws.com:/home/ec2-user/docker_exp\n",
    "</code>\n",
    "\n",
    "### Docker installation and running\n",
    "    \n",
    "### Usage of docker-composer and starting all the tools\n",
    "\n",
    "- Commands to install Docker\n",
    "\n",
    "<code>sudo yum update -y</code>\n",
    "<code><br/>sudo yum install docker</code>\n",
    "<code><br/>sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose</code>\n",
    "<code><br/>sudo chmod +x /usr/local/bin/docker-compose</code>\n",
    "<code><br/>sudo gpasswd -a $USER docker</code>\n",
    "<code><br/>newgrp docker</code>\n",
    "<br/>Start Docker: <code>sudo systemctl start docker</code>\n",
    "<br/>Stop Docker: <code>sudo systemctl stop docker</code>\n",
    "\n",
    "- How to access tools in local machine <br/>\n",
    "    List Docker containers running: <code>docker ps</code><br/>\n",
    "    CLI access in Docker container: <code>docker exec -i -t kafka bash</code><br/>\n",
    "    NiFi at: http://localhost:2080/nifi/ <br/>\n",
    "    Jupyter Lab at: http://localhost:4888/lab? <br/>\n",
    "    HDFS at: http://localhost:50070/\n",
    "    Dash Application at: http://localhost:8050/ (this will be available when executed log_visualizer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf1ffd",
   "metadata": {},
   "source": [
    "## <font color=blue>Log Preprocessing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec854b",
   "metadata": {},
   "source": [
    "- Walk thru <a href=\"https://www.w3.org/Daemon/User/Config/Logging.html#common-logfile-format\"> Common Log Format </a>\n",
    "- Parsing Log file\n",
    "- Data Cleaning\n",
    "- Fix the rows with null content_size\n",
    "- Formating timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f328fa2",
   "metadata": {
    "id": "6523b815"
   },
   "source": [
    "## <font color=blue>Extraction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ec6b2",
   "metadata": {
    "id": "c85d6007"
   },
   "source": [
    "### Download Dataset\n",
    "- <a href = https://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html> Actual NASA Logs </a>\n",
    "- <a href = https://www.kaggle.com/souhagaa/nasa-access-log-dataset-1995/download> Kaggle </a>\n",
    "- <code>docker exec -i -t nifi bash </br>mkdir -p nasa_logs && cp /opt/workspace/nifi/InputData/data.csv nasa_logs/data.csv</code>\n",
    "\n",
    "### Nifi\n",
    "- Processor\n",
    "- Connection\n",
    "\n",
    "### Kafka\n",
    "- Topic\n",
    "- Producer\n",
    "- Consumer\n",
    "\n",
    "### Topic\n",
    "- Topic creation through CLI\n",
    "\n",
    "#### Commands\n",
    "<code>docker ps</code> to get kafka container name\n",
    "\n",
    "<code>docker exec -i -t kafka bash</code> enter into kafka CLI\n",
    "\n",
    "<code>kafka-topics.sh --create --topic nasa_logs_demo --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:2181</code> creation of topic named nasa_logs_demo\n",
    "\n",
    "<code>kafka-topics.sh --list --bootstrap-server localhost:29092</code> list topics\n",
    "\n",
    "<code>kafka-topics.sh --describe --topic nasa_logs_demo --zookeeper zookeeper:2181</code> describe topic\n",
    "\n",
    "<code>kafka-topics.sh --delete --topic nasa_logs_demo --zookeeper zookeeper:2181</code> delete topic\n",
    "\n",
    "<code>kafka-console-consumer.sh --bootstrap-server localhost:29092 --topic nasa_logs_demo --from-beginning --max-messages 30</code> consume/read data from topic\n",
    "\n",
    "### Streaming data from file system using NiFi\n",
    "- Goto http://localhost:2080/nifi/\n",
    "- Nifi Setup\n",
    "- Publish log data via NiFi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0805c5",
   "metadata": {
    "id": "402a9a13"
   },
   "source": [
    "## <font color=blue>Transformation and Load</font>\n",
    "\n",
    "### Cassandra and HDFS set up\n",
    "- Create namespace and table in Cassandra\n",
    "- CQL Commands\n",
    "<code>\n",
    "    docker exec -i -t cassandra bash\n",
    "    cqlsh -u cassandra -p cassandra\n",
    "    CREATE KEYSPACE IF NOT EXISTS LogAnalysis WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};\n",
    "    CREATE TABLE IF NOT EXISTS LogAnalysis.NASALog (host text , time text , method text , url text , response text , bytes text, extension text, time_added text,PRIMARY KEY (host));\n",
    "    truncate table LogAnalysis.NASALog;\n",
    "</code>\n",
    "- Create a folder in HDFS\n",
    "<code>\n",
    "    docker exec -i -t namenode bash\n",
    "    hdfs dfs -mkdir -p /output1/nasa_logs/\n",
    "    http://localhost:50070/\n",
    "</code>\n",
    "\n",
    "### Read Streaming Data and Cleansing\n",
    "- Schema creation\n",
    "- Reading data from Kafka as Streaming Dataframe\n",
    "- Extraction and cleansing of Log data\n",
    "\n",
    "### Data Loading\n",
    "- Continuous data load to Cassandra\n",
    "- Writing data to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e474a",
   "metadata": {
    "id": "ca51eb02"
   },
   "source": [
    "## <font color=blue>Visualization</font>\n",
    "- Scatter graph and Table definition with intervals using Python Plotly and Dash\n",
    "- Graph and Table app call-back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c16bca",
   "metadata": {
    "id": "83dad3d3"
   },
   "source": [
    "## <font color=blue>Code walkthrough</font>\n",
    "- Log Listener\n",
    "- Log Visualizer"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "presentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
